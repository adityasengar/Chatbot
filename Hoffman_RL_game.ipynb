{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO2y2TQriX1DpiieMBDD1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityasengar/Chatbot/blob/main/Hoffman_RL_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZZFp6iF-tX0N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# World dimensions\n",
        "WORLD_SIZE = 5\n",
        "\n",
        "# Rewards\n",
        "REWARD_MEET = 100\n",
        "REWARD_MOVE = -1\n",
        "REWARD_MEET_AFTER_HUMAN = 50\n",
        "\n",
        "# Hyperparameters for Q-Learning\n",
        "EPSILON = 0.1\n",
        "ALPHA = 0.5\n",
        "GAMMA = 0.9\n",
        "EPISODES = 5000\n",
        "\n",
        "# Actions: Up, Down, Left, Right\n",
        "ACTIONS = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
        "\n",
        "# Q-Tables for the Humans and the Robots\n",
        "Q_human1 = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))\n",
        "Q_human2 = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))\n",
        "Q_robot1 = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))\n",
        "Q_robot2 = np.zeros((WORLD_SIZE, WORLD_SIZE, len(ACTIONS)))\n",
        "\n",
        "def step(state, action):\n",
        "    next_state = (max(0, min(WORLD_SIZE - 1, state[0] + action[0])),\n",
        "                  max(0, min(WORLD_SIZE - 1, state[1] + action[1])))\n",
        "    return next_state\n",
        "\n",
        "def get_action(state, Q_table):\n",
        "    if np.random.rand() < EPSILON:\n",
        "        return np.random.choice(len(ACTIONS))\n",
        "    else:\n",
        "        return np.argmax(Q_table[state[0], state[1], :])\n",
        "\n",
        "def learn(state, action, reward, next_state, Q_table):\n",
        "    target = reward + GAMMA * np.max(Q_table[next_state[0], next_state[1], :])\n",
        "    Q_table[state[0], state[1], action] += ALPHA * (target - Q_table[state[0], state[1], action])\n",
        "\n",
        "for _ in range(EPISODES):\n",
        "    # Start positions for Humans and Robots\n",
        "    human1 = (np.random.randint(WORLD_SIZE), np.random.randint(WORLD_SIZE))\n",
        "    human2 = (np.random.randint(WORLD_SIZE), np.random.randint(WORLD_SIZE))\n",
        "    robot1 = (np.random.randint(WORLD_SIZE), np.random.randint(WORLD_SIZE))\n",
        "    robot2 = (np.random.randint(WORLD_SIZE), np.random.randint(WORLD_SIZE))\n",
        "\n",
        "    met_human = False\n",
        "\n",
        "    while True:\n",
        "        action_human1 = get_action(human1, Q_human1)\n",
        "        action_human2 = get_action(human2, Q_human2)\n",
        "        action_robot1 = get_action(robot1, Q_robot1)\n",
        "        action_robot2 = get_action(robot2, Q_robot2)\n",
        "\n",
        "        next_human1 = step(human1, ACTIONS[action_human1])\n",
        "        next_human2 = step(human2, ACTIONS[action_human2])\n",
        "        next_robot1 = step(robot1, ACTIONS[action_robot1])\n",
        "        next_robot2 = step(robot2, ACTIONS[action_robot2])\n",
        "\n",
        "        # Check for AI creation\n",
        "        if next_human1 == next_robot1 or next_human1 == next_robot2 or next_human2 == next_robot1 or next_human2 == next_robot2:\n",
        "            reward = REWARD_MEET_AFTER_HUMAN if met_human else REWARD_MEET\n",
        "            learn(human1, action_human1, reward, next_human1, Q_human1)\n",
        "            learn(human2, action_human2, reward, next_human2, Q_human2)\n",
        "            learn(robot1, action_robot1, reward, next_robot1, Q_robot1)\n",
        "            learn(robot2, action_robot2, reward, next_robot2, Q_robot2)\n",
        "            break\n",
        "        # Check for human-human meeting\n",
        "        elif next_human1 == next_human2:\n",
        "            met_human = True\n",
        "\n",
        "        reward = REWARD_MOVE\n",
        "        learn(human1, action_human1, reward, next_human1, Q_human1)\n",
        "        learn(human2, action_human2, reward, next_human2, Q_human2)\n",
        "        learn(robot1, action_robot1, reward, next_robot1, Q_robot1)\n",
        "        learn(robot2, action_robot2, reward, next_robot2, Q_robot2)\n",
        "\n",
        "        human1 = next_human1\n",
        "        human2 = next_human2\n",
        "        robot1 = next_robot1\n",
        "        robot2 = next_robot2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_human1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcL-f-UYt0Ky",
        "outputId": "bbb88731-38a3-496b-c334-a2c37fa899e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[144.27289585, 251.38546754, 144.24176647, 141.93603096],\n",
              "        [168.58546683, 304.50678394, 150.80801561, 196.67405767],\n",
              "        [ 94.41199885, 306.31346668, 158.0686772 , 148.12656759],\n",
              "        [153.0218114 , 133.8526206 , 144.36472339, 292.71247828],\n",
              "        [138.11002271, 137.08701616, 135.48962515, 349.99989042]],\n",
              "\n",
              "       [[150.10026008, 230.67978235, 147.66347469, 149.99837496],\n",
              "        [150.66785305, 148.02628198, 154.57694166, 294.51078501],\n",
              "        [226.84838977, 190.18182051, 151.11148699, 415.46770058],\n",
              "        [333.08987045, 176.72313176, 170.72473479, 140.51948723],\n",
              "        [204.88102746, 140.01308902, 142.3585863 , 422.02534047]],\n",
              "\n",
              "       [[226.82511321, 324.90050374, 151.65191511, 177.20281841],\n",
              "        [178.62653686, 376.93351989, 151.48003298, 239.0216206 ],\n",
              "        [209.58662831, 306.54744829, 146.42950025, 379.70509984],\n",
              "        [269.96507627, 274.06596853, 224.54950707, 357.71923881],\n",
              "        [273.75776577, 173.41132489, 289.26405776, 441.34913294]],\n",
              "\n",
              "       [[149.37884468, 150.82030893, 304.8745989 , 165.79382539],\n",
              "        [189.97628242, 143.87969868, 324.05449467, 152.53611429],\n",
              "        [182.40539797, 371.28655669, 251.37267994, 218.9671506 ],\n",
              "        [144.4450825 , 428.23540525, 284.32932008, 285.11475933],\n",
              "        [281.59161148, 321.29669814, 415.52280963, 291.27339727]],\n",
              "\n",
              "       [[166.06027954, 226.82721121, 149.40106513, 148.84331484],\n",
              "        [148.32642214, 186.74791792, 297.20516147, 150.03562534],\n",
              "        [149.35355876, 217.51987541, 380.00886586, 144.1649399 ],\n",
              "        [172.48196343, 147.12409857, 435.8208462 , 171.75883522],\n",
              "        [212.64754452, 200.67157276, 428.04012539, 146.79221864]]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SxqwoArLuGZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}